{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temperature Scaling Practice with netcal\n",
    "Learn how to apply temperature scaling to calibrate a model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "if not hasattr(np, 'infty'):\n",
    "    np.infty = np.inf\n",
    "import matplotlib.pyplot as plt\n",
    "from netcal.scaling import TemperatureScaling\n",
    "from netcal.metrics import ECE\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEMPERATURE SCALING PRACTICE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate synthetic miscalibrated predictions\n",
    "print(\"\\n1. Generating synthetic miscalibrated predictions...\")\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# True labels (binary)\n",
    "y_true = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "# Generate probabilities directly (easier to control)\n",
    "y_pred_proba_uncalibrated = np.zeros(n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if np.random.rand() < 0.80:  # 80% accuracy\n",
    "        # Correct prediction - make it overconfident\n",
    "        if y_true[i] == 1:\n",
    "            # Should be high, but make it TOO high (overconfident)\n",
    "            y_pred_proba_uncalibrated[i] = np.random.uniform(0.75, 0.98)\n",
    "        else:\n",
    "            # Should be low, but make it TOO low (overconfident)\n",
    "            y_pred_proba_uncalibrated[i] = np.random.uniform(0.02, 0.25)\n",
    "    else:\n",
    "        # Incorrect prediction - still overconfident (this creates miscalibration!)\n",
    "        if y_true[i] == 1:\n",
    "            # Wrong class, but confident\n",
    "            y_pred_proba_uncalibrated[i] = np.random.uniform(0.02, 0.25)\n",
    "        else:\n",
    "            # Wrong class, but confident\n",
    "            y_pred_proba_uncalibrated[i] = np.random.uniform(0.75, 0.98)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (y_pred_proba_uncalibrated > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_pred == y_true).mean()\n",
    "print(f\"  Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Show confidence distribution\n",
    "high_conf_positive = (y_pred_proba_uncalibrated > 0.8).sum()\n",
    "mid_high_conf = ((y_pred_proba_uncalibrated > 0.6) & (y_pred_proba_uncalibrated <= 0.8)).sum()\n",
    "mid_low_conf = ((y_pred_proba_uncalibrated >= 0.2) & (y_pred_proba_uncalibrated < 0.4)).sum()\n",
    "low_conf_negative = (y_pred_proba_uncalibrated < 0.2).sum()\n",
    "\n",
    "print(f\"  Very high confidence (>0.8): {high_conf_positive}/{n_samples} ({high_conf_positive/n_samples*100:.1f}%)\")\n",
    "print(f\"  Medium-high (0.6-0.8): {mid_high_conf}/{n_samples} ({mid_high_conf/n_samples*100:.1f}%)\")\n",
    "print(f\"  Medium-low (0.2-0.4): {mid_low_conf}/{n_samples} ({mid_low_conf/n_samples*100:.1f}%)\")\n",
    "print(f\"  Very low confidence (<0.2): {low_conf_negative}/{n_samples} ({low_conf_negative/n_samples*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate ECE before calibration (on full dataset for initial assessment)\n",
    "print(\"\\n2. Calibration BEFORE temperature scaling...\")\n",
    "\n",
    "ece_metric = ECE(bins=10)\n",
    "ece_before_full = ece_metric.measure(y_pred_proba_uncalibrated, y_true)\n",
    "\n",
    "print(f\"  ECE (full dataset): {ece_before_full:.4f}\")\n",
    "if ece_before_full < 0.05:\n",
    "    print(f\"  Status: Excellent calibration ✓\")\n",
    "elif ece_before_full < 0.10:\n",
    "    print(f\"  Status: Good calibration\")\n",
    "elif ece_before_full < 0.15:\n",
    "    print(f\"  Status: Acceptable calibration\")\n",
    "else:\n",
    "    print(f\"  Status: Poor calibration - needs fixing ✗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a28346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply temperature scaling\n",
    "print(\"\\n3. Applying temperature scaling...\")\n",
    "\n",
    "# Split data: use first 70% for calibration, last 30% for testing\n",
    "n_cal = int(0.7 * n_samples)\n",
    "\n",
    "# For netcal with BINARY classification, we need 2D array\n",
    "# Column 0: probability of class 0, Column 1: probability of class 1\n",
    "y_proba_2d = np.column_stack([\n",
    "    1 - y_pred_proba_uncalibrated,  # P(class=0)\n",
    "    y_pred_proba_uncalibrated        # P(class=1)\n",
    "])\n",
    "\n",
    "# Split into calibration and test sets\n",
    "y_proba_cal = y_proba_2d[:n_cal]\n",
    "y_true_cal = y_true[:n_cal]\n",
    "\n",
    "y_proba_test = y_proba_2d[n_cal:]\n",
    "y_true_test = y_true[n_cal:]\n",
    "y_pred_proba_test = y_pred_proba_uncalibrated[n_cal:]\n",
    "\n",
    "# Calculate ECE on test set BEFORE calibration (for fair comparison)\n",
    "ece_before_test = ece_metric.measure(y_pred_proba_test, y_true_test)\n",
    "print(f\"  ECE on test set (before calibration): {ece_before_test:.4f}\")\n",
    "\n",
    "# Initialize temperature scaling\n",
    "temperature = TemperatureScaling()\n",
    "\n",
    "# Fit on calibration set (finds optimal temperature T)\n",
    "# netcal expects PROBABILITIES as input, not logits\n",
    "temperature.fit(y_proba_cal, y_true_cal)\n",
    "\n",
    "temp_value = temperature.temperature.item() if hasattr(temperature.temperature, 'item') else float(temperature.temperature)\n",
    "print(f\"  Optimal temperature: {temp_value:.4f}\")\n",
    "print(f\"  Interpretation: {'Model is overconfident (T>1 softens predictions)' if temp_value > 1 else 'Model is underconfident (T<1 sharpens predictions)'}\")\n",
    "\n",
    "# Apply to test set\n",
    "y_pred_proba_calibrated_full = temperature.transform(y_proba_test)\n",
    "# Check if it's 1D or 2D\n",
    "if y_pred_proba_calibrated_full.ndim == 1:\n",
    "    y_pred_proba_calibrated = y_pred_proba_calibrated_full\n",
    "else:\n",
    "    y_pred_proba_calibrated = y_pred_proba_calibrated_full[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate ECE after calibration\n",
    "print(\"\\n4. Calibration AFTER temperature scaling...\")\n",
    "\n",
    "ece_after = ece_metric.measure(y_pred_proba_calibrated, y_true_test)\n",
    "\n",
    "print(f\"  ECE on test set (after calibration): {ece_after:.4f}\")\n",
    "if ece_after < 0.05:\n",
    "    print(f\"  Status: Excellent calibration ✓\")\n",
    "elif ece_after < 0.10:\n",
    "    print(f\"  Status: Good calibration\")\n",
    "else:\n",
    "    print(f\"  Status: Acceptable calibration\")\n",
    "\n",
    "improvement = ece_before_test - ece_after  # Compare both on test set\n",
    "improvement_pct = (improvement / ece_before_test) * 100 if ece_before_test > 0 else 0\n",
    "\n",
    "print(f\"\\n  ECE Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n",
    "if improvement > 0:\n",
    "    print(f\"  ✓ Calibration improved!\")\n",
    "else:\n",
    "    print(f\"  ⚠ Calibration got worse (unexpected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c32987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize calibration\n",
    "print(\"\\n5. Creating calibration visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Reliability diagram - BEFORE\n",
    "frac_pos_before, mean_pred_before = calibration_curve(\n",
    "    y_true_test, y_pred_proba_test, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect calibration')\n",
    "axes[0, 0].plot(mean_pred_before, frac_pos_before, 'o-', linewidth=2, \n",
    "                markersize=8, color='coral', label='Model (before)')\n",
    "axes[0, 0].set_xlabel('Mean Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Fraction of Positives', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title(f'Before Temperature Scaling\\nECE = {ece_before_test:.4f}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Reliability diagram - AFTER\n",
    "frac_pos_after, mean_pred_after = calibration_curve(\n",
    "    y_true_test, y_pred_proba_calibrated, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect calibration')\n",
    "axes[0, 1].plot(mean_pred_after, frac_pos_after, 'o-', linewidth=2,\n",
    "                markersize=8, color='steelblue', label='Model (after)')\n",
    "axes[0, 1].set_xlabel('Mean Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Fraction of Positives', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title(f'After Temperature Scaling\\nECE = {ece_after:.4f}',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Confidence histogram - BEFORE\n",
    "axes[1, 0].hist(y_pred_proba_test, bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1, 0].axvline(0.5, color='black', linestyle='--', alpha=0.5, label='Decision threshold')\n",
    "axes[1, 0].set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Confidence Distribution (Before)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Confidence histogram - AFTER\n",
    "axes[1, 1].hist(y_pred_proba_calibrated, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1, 1].axvline(0.5, color='black', linestyle='--', alpha=0.5, label='Decision threshold')\n",
    "axes[1, 1].set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Confidence Distribution (After)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Temperature Scaling Effect on Calibration', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create outputs directory if needed\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "plt.savefig('outputs/netcal_practice_calibration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"  ✓ Saved calibration plots to outputs/netcal_practice_calibration.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de959c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Effect on confidence\n",
    "print(\"\\n6. Effect on prediction confidence...\")\n",
    "\n",
    "print(f\"\\nBefore calibration (test set):\")\n",
    "print(f\"  Mean confidence: {y_pred_proba_test.mean():.4f}\")\n",
    "print(f\"  Max confidence: {y_pred_proba_test.max():.4f}\")\n",
    "print(f\"  Min confidence: {y_pred_proba_test.min():.4f}\")\n",
    "print(f\"  Std dev: {y_pred_proba_test.std():.4f}\")\n",
    "\n",
    "print(f\"\\nAfter calibration (test set):\")\n",
    "print(f\"  Mean confidence: {y_pred_proba_calibrated.mean():.4f}\")\n",
    "print(f\"  Max confidence: {y_pred_proba_calibrated.max():.4f}\")\n",
    "print(f\"  Min confidence: {y_pred_proba_calibrated.min():.4f}\")\n",
    "print(f\"  Std dev: {y_pred_proba_calibrated.std():.4f}\")\n",
    "\n",
    "# Check if accuracy is preserved - calculate both on test set\n",
    "y_pred_before = (y_pred_proba_test > 0.5).astype(int)\n",
    "y_pred_after = (y_pred_proba_calibrated > 0.5).astype(int)\n",
    "accuracy_before = (y_pred_before == y_true_test).mean()\n",
    "accuracy_after = (y_pred_after == y_true_test).mean()\n",
    "\n",
    "print(f\"\\nAccuracy comparison (both on test set):\")\n",
    "print(f\"  Before: {accuracy_before*100:.2f}%\")\n",
    "print(f\"  After:  {accuracy_after*100:.2f}%\")\n",
    "print(f\"  Change: {(accuracy_after - accuracy_before)*100:.2f}%\")\n",
    "print(f\"  ✓ Accuracy preserved!\" if abs(accuracy_after - accuracy_before) < 0.01 else \"  ⚠ Accuracy changed (should be identical)\")\n",
    "\n",
    "# Additional diagnostic\n",
    "if abs(accuracy_after - accuracy_before) > 0.01:\n",
    "    print(f\"\\n  NOTE: Accuracy should be identical. If changed, it means the threshold (0.5)\")\n",
    "    print(f\"        crossed for some predictions during calibration.\")\n",
    "    num_changed = (y_pred_before != y_pred_after).sum()\n",
    "    print(f\"        Number of predictions that changed: {num_changed}/{len(y_pred_before)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Temperature scaling improves calibration (lowers ECE)\")\n",
    "print(\"2. T > 1: Model is overconfident, predictions get softened\")\n",
    "print(\"3. T < 1: Model is underconfident, predictions get sharpened\")\n",
    "print(\"4. Accuracy should be preserved (same predicted classes)\")\n",
    "print(\"5. Simple: just one parameter to optimize on calibration set\")\n",
    "print(\"6. Fast: can be done on validation set after training\")\n",
    "print(\"7. netcal expects 2D probability arrays as input\")\n",
    "print(\"\\n✓ Temperature scaling practice complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin-cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
