{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8883ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALIBRATION IN NEURAL NETWORKS\n",
      "======================================================================\n",
      "\n",
      "✓ Calibration theory notes complete\n",
      "\n",
      "Key takeaways:\n",
      "  1. Calibration: predicted probability = actual frequency\n",
      "  2. ECE measures calibration quality (lower is better)\n",
      "  3. Temperature scaling: simple, effective post-hoc method\n",
      "  4. Modern networks are typically overconfident\n",
      "  5. Always check calibration in medical AI applications\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calibration Theory Notes\n",
    "Based on Guo et al. (2017) - \"On Calibration of Modern Neural Networks\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CALIBRATION IN NEURAL NETWORKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\"\"\"\n",
    "## What is Calibration?\n",
    "\n",
    "### Definition\n",
    "A model is **calibrated** if its predicted probabilities match the actual frequencies:\n",
    "  P(Y=1 | confidence=c) ≈ c\n",
    "\n",
    "Example:\n",
    "- Model predicts 100 samples with 80% confidence\n",
    "- If 80 of them are actually correct → CALIBRATED ✓\n",
    "- If only 60 are correct → OVERCONFIDENT ✗\n",
    "- If 95 are correct → UNDERCONFIDENT ✗\n",
    "\n",
    "### Why It Matters for Medical AI\n",
    "1. **Trust**: Doctors need accurate confidence scores\n",
    "2. **Decision Making**: Confidence determines when to defer\n",
    "3. **Regulatory**: EU MDR requires transparency\n",
    "4. **Safety**: Overconfident wrong predictions are dangerous\n",
    "\n",
    "### Perfect vs Imperfect Calibration\n",
    "\n",
    "Perfect Calibration:\n",
    "- 90% confident predictions → 90% correct\n",
    "- 50% confident predictions → 50% correct\n",
    "- Reliability diagram follows diagonal line\n",
    "\n",
    "Miscalibrated (typical modern networks):\n",
    "- 90% confident predictions → only 70% correct (OVERCONFIDENT)\n",
    "- Reliability diagram deviates from diagonal\n",
    "\n",
    "## Expected Calibration Error (ECE)\n",
    "\n",
    "### Formula\n",
    "ECE = Σ (|acc(B_m) - conf(B_m)| × |B_m| / n)\n",
    "\n",
    "Where:\n",
    "- B_m = bins of predictions grouped by confidence\n",
    "- acc(B_m) = accuracy within bin m\n",
    "- conf(B_m) = average confidence in bin m\n",
    "- n = total number of samples\n",
    "\n",
    "### Interpretation\n",
    "- ECE ∈ [0, 1]\n",
    "- **ECE < 0.05**: Excellent calibration ✓\n",
    "- **ECE < 0.10**: Good calibration\n",
    "- **ECE < 0.15**: Acceptable calibration\n",
    "- **ECE ≥ 0.15**: Poor calibration, needs fixing\n",
    "\n",
    "### How to Calculate\n",
    "1. Group predictions into bins (typically 10-15 bins)\n",
    "2. For each bin:\n",
    "   - Calculate average confidence\n",
    "   - Calculate actual accuracy\n",
    "   - Find absolute difference\n",
    "3. Weight by number of samples in bin\n",
    "4. Sum across all bins\n",
    "\n",
    "## Temperature Scaling\n",
    "\n",
    "### Concept\n",
    "Scale logits by temperature T before softmax:\n",
    "\n",
    "Original:  p_i = exp(z_i) / Σ exp(z_j)\n",
    "Scaled:    p_i = exp(z_i/T) / Σ exp(z_j/T)\n",
    "\n",
    "Where:\n",
    "- z_i = logit for class i\n",
    "- T = temperature parameter (T > 0)\n",
    "\n",
    "### Effect of Temperature\n",
    "- T = 1: No change (original model)\n",
    "- T > 1: Softer probabilities (less confident)\n",
    "- T < 1: Harder probabilities (more confident)\n",
    "\n",
    "### Why It Works\n",
    "- **Single parameter**: Easy to optimize\n",
    "- **Post-hoc**: Applied after training\n",
    "- **No retraining**: Use validation set to find T\n",
    "- **Preserves accuracy**: Doesn't change predicted class\n",
    "- **Fixes overconfidence**: Common in modern networks\n",
    "\n",
    "### Optimization\n",
    "Use validation set to find T that minimizes NLL (Negative Log-Likelihood):\n",
    "\n",
    "T* = argmin_T  NLL(validation_set)\n",
    "\n",
    "Typically: 1 < T < 5 for overconfident networks\n",
    "\n",
    "## Why Modern Networks are Miscalibrated\n",
    "\n",
    "### Key Findings from Paper\n",
    "1. **Model Capacity**: Larger models → more overconfident\n",
    "2. **Batch Normalization**: Contributes to miscalibration\n",
    "3. **Modern Architectures**: ResNet, DenseNet more miscalibrated than LeNet\n",
    "4. **Dataset Size**: Small datasets → worse calibration\n",
    "\n",
    "### Practical Implication\n",
    "ALWAYS apply calibration to modern neural networks, especially:\n",
    "- Deep networks (>50 layers)\n",
    "- Networks with batch normalization\n",
    "- Medical AI applications\n",
    "- When using ImageNet pretrained weights\n",
    "\n",
    "## Reliability Diagrams\n",
    "\n",
    "### How to Read\n",
    "- X-axis: Predicted confidence (binned)\n",
    "- Y-axis: Actual accuracy\n",
    "- Diagonal line: Perfect calibration\n",
    "- Gap from diagonal: Calibration error\n",
    "\n",
    "### Patterns\n",
    "- Points above diagonal: Underconfident\n",
    "- Points below diagonal: Overconfident (common)\n",
    "- Close to diagonal: Well-calibrated\n",
    "\n",
    "### Best Practices\n",
    "- Use 10-15 bins\n",
    "- Weight by number of samples\n",
    "- Show both before/after calibration\n",
    "- Include confidence histogram\n",
    "\n",
    "## Summary for Our Project\n",
    "\n",
    "### Phase 1 (Baseline)\n",
    "1. Train EfficientNet-B0 (will be overconfident)\n",
    "2. Calculate ECE on validation set\n",
    "3. Apply temperature scaling\n",
    "4. Calculate ECE after scaling\n",
    "5. Target: ECE < 0.05\n",
    "\n",
    "### Phases 2-3 (Uncertainty)\n",
    "1. MC Dropout provides epistemic uncertainty\n",
    "2. Temperature scaling provides calibrated probabilities\n",
    "3. Use BOTH together for best results\n",
    "\n",
    "### Key Metrics to Track\n",
    "- Accuracy (classification performance)\n",
    "- AUROC (discrimination ability)\n",
    "- ECE (calibration quality)\n",
    "- Reliability diagram (visualization)\n",
    "\n",
    "## References\n",
    "Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017).\n",
    "On Calibration of Modern Neural Networks.\n",
    "ICML 2017.\n",
    "https://arxiv.org/abs/1706.04599\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n✓ Calibration theory notes complete\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  1. Calibration: predicted probability = actual frequency\")\n",
    "print(\"  2. ECE measures calibration quality (lower is better)\")\n",
    "print(\"  3. Temperature scaling: simple, effective post-hoc method\")\n",
    "print(\"  4. Modern networks are typically overconfident\")\n",
    "print(\"  5. Always check calibration in medical AI applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1918336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Monte Carlo Dropout (Preview)\n",
    "\n",
    "### High-Level Concept\n",
    "MC Dropout keeps dropout ACTIVE at test time:\n",
    "- Run multiple forward passes (T times)\n",
    "- Each pass uses different dropout mask\n",
    "- Variance across passes = epistemic uncertainty\n",
    "\n",
    "### What It Provides\n",
    "- Epistemic (model) uncertainty\n",
    "- Confidence intervals\n",
    "- Out-of-distribution detection\n",
    "- Deferral capability\n",
    "\n",
    "### Difference from Temperature Scaling\n",
    "\n",
    "**Temperature Scaling:**\n",
    "- Fixes calibration (probabilities match frequencies)\n",
    "- Single forward pass\n",
    "- Fast\n",
    "- No uncertainty quantification\n",
    "\n",
    "**MC Dropout:**\n",
    "- Quantifies epistemic uncertainty\n",
    "- Multiple forward passes (slower)\n",
    "- Can detect when model \"doesn't know\"\n",
    "- Provides prediction variance\n",
    "\n",
    "### Why Use Both?\n",
    "- Temperature Scaling → Calibrated probabilities\n",
    "- MC Dropout → Uncertainty estimates\n",
    "- Combined → Calibrated probabilities WITH uncertainty\n",
    "\n",
    "### Phase 2 Focus\n",
    "We'll implement and analyze MC Dropout in detail:\n",
    "- How many forward passes (T)?\n",
    "- What uncertainty metrics?\n",
    "- How to visualize?\n",
    "- When to defer to expert?\n",
    "\n",
    "For now: Just know it exists and we'll use it later.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n✓ MC Dropout overview added\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin-cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
